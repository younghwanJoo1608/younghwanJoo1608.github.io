---
title:  "1.2 Multivariable Linear Regression"
excerpt: 

categories:
  - Robot
tags:
  - [Machine Learning]

toc: true
toc_sticky: true
 
date: 2022-07-05
last_modified_at: 2022-07-05

use_math: true
published: true
---

<br>

지훈이는 오늘도 또 싱글벙글 게임을 켰다. 어제는 슬라임만 잡았지만 오늘은 다르다. 슬라임을 17마리, 고블린을 25마리, 늑대를 10마리 잡았더니 1695원을 벌었다. 

|Slime ($x_1$)|Goblin ($x_2$)|Wolf ($x_3$)|Money ($y$)|
|---|---|---|---|
|17|25|10|1985|
|36|7|31|3185|
|22|23|47|4285|
|5|21|11|1545|
|45|17|41|4405|

슬라임을 $x_1$마리, 고블린을 $x_2$마리, 늑대를 $x_3$마리 사냥했을 때 얼마를 벌 수 있는지 예측하고 싶다.

<script src="https://gist.github.com/younghwanjoo1608/cd532dc35baf2de9019a47e4d883b206.js"></script>

<br>

***

### 1.1.2 Multivariable Linear Regression

Hypothesis function이 다음과 같았던 것을 기억하자.

$$H(x) = Wx + b$$

따라서, 입력 변수가 3개이면 weight도 3개의 성분으로 이루어져 있어야 한다.

$$H(x) = w_1x_1 + w_2x_2 + w_3x_3 + b$$

만약 입력 변수가 1000개이면...

<br>

`matmul()` 함수는 두 array의 matrix product를 계산한다. 훨씬 간결하며 입력 변수의 개수가 바뀌어도 코드를 바꿀 필요가 없어진다.

이번에도 cost 함수는 MSE를 사용하자. 기존의 simple linear regression과 동일한 방식이다.

$$ cost(W, b) = \frac{1}{m} \sum_{i=1}^m (H(x^{(i)}) - y^{(i)})^2$$

<script src="https://gist.github.com/younghwanjoo1608/b00a8a1a237856f16b72923d3d6426e4.js"></script>

```cpp
 - matmul() : 텐서의 행렬곱을 계산
 - torch.mean() : 텐서의 원소 간의 평균을 계산
---
 - Result : 
  Epoch  100/1000 hypothesis: tensor([2016.3593, 3206.3818, 4227.2891, 1537.0814, 4428.6533]), W: tensor([32.8706, 35.3919, 57.2540]), b: 0.0000, Cost: 1078.664307
  Epoch  200/1000 hypothesis: tensor([1996.3647, 3188.8352, 4270.0488, 1545.6074, 4410.2295]), W: tensor([30.7304, 35.2673, 59.2133]), b: 0.0000, Cost: 79.024063
  Epoch  300/1000 hypothesis: tensor([1988.3540, 3185.9561, 4280.8555, 1545.3354, 4406.3931]), W: tensor([30.2016, 35.0845, 59.7772]), b: 0.0000, Cost: 6.278736
  Epoch  400/1000 hypothesis: tensor([1985.9556, 3185.2627, 4283.8340, 1545.1041, 4405.3887]), W: tensor([30.0567, 35.0244, 59.9370]), b: 0.0000, Cost: 0.500723
  Epoch  500/1000 hypothesis: tensor([1985.2705, 3185.0737, 4284.6709, 1545.0300, 4405.1094]), W: tensor([30.0160, 35.0069, 59.9822]), b: 0.0000, Cost: 0.039957
  Epoch  600/1000 hypothesis: tensor([1985.0764, 3185.0210, 4284.9067, 1545.0085, 4405.0308]), W: tensor([30.0045, 35.0020, 59.9950]), b: 0.0000, Cost: 0.003199
  Epoch  700/1000 hypothesis: tensor([1985.0217, 3185.0059, 4284.9736, 1545.0026, 4405.0088]), W: tensor([30.0013, 35.0006, 59.9986]), b: 0.0000, Cost: 0.000257
  Epoch  800/1000 hypothesis: tensor([1985.0059, 3185.0017, 4284.9922, 1545.0005, 4405.0024]), W: tensor([30.0004, 35.0002, 59.9996]), b: 0.0000, Cost: 0.000021
  Epoch  900/1000 hypothesis: tensor([1985.0024, 3185.0000, 4284.9980, 1545.0006, 4405.0005]), W: tensor([30.0001, 35.0001, 59.9999]), b: 0.0000, Cost: 0.000002
  Epoch 1000/1000 hypothesis: tensor([1985.0024, 3185.0000, 4284.9980, 1545.0006, 4405.0005]), W: tensor([30.0001, 35.0001, 59.9999]), b: 0.0000, Cost: 0.000002
```

epoch가 진행될수록 cost는 점점 작아지고, $H(x)$는 $y$에 가까워지는 것을 알 수 있다.

<br>

***

### 1.1.2 `nn.Module`

모델이 커지고 복잡해지면 weight $W$와 bias $b$를 일일이 적어주는 것이 불편해질 수 있다. Pytorch는 `nn.module`을 상속시켜서 모델을 생성하는 기능을 제공한다.

다음 두 코드를 비교해 보자.

<script src="https://gist.github.com/younghwanjoo1608/3d2627c72448e513a4325ed415804b8b.js"></script>

<br>

<script src="https://gist.github.com/younghwanjoo1608/88fbfa8d68320855832c5fa3b0e5da23.js"></script>

```cpp
 - nn.Linear(3, 1) : 입력 차원이 3, 출력 차원이 1인 Linear model을 생성
 - forward(self, x) : hypothesis 계산 역할을 해 주는 함수
```

`nn.Module`에서 gradient의 계산(`backward()`)은 pytorch가 알아서 해 준다.

<br>

***

### 1.1.3 `torch.nn.functional`

Pytorch는 여러 loss function을 제공한다. 이로부터 기존의 모델에 쓰인 loss를 다른 loss로 쉽게 교체할 수 있다.

<script src="https://gist.github.com/younghwanjoo1608/0593a66cadd41918f8de5a567759e082.js"></script>

<br>

<script src="https://gist.github.com/younghwanjoo1608/8f4d5f578ba11e348866dd2e9a47f90d.js"></script>

<br>

***

위 내용들을 토대로 Multivariable linear regression의 코드를 다시 만들어 보자.

<script src="https://gist.github.com/younghwanjoo1608/2a53c786aee5650eff32693fa9ab2f75.js"></script>

```cpp
 - Result : 
Epoch  100/1000 hypothesis: tensor([2017.6698, 3206.9531, 4225.7388, 1537.6654, 4428.8535]), W: tensor([32.9236, 35.3904, 57.1705]), b: 1.2638, Cost: 1136.787109
Epoch  200/1000 hypothesis: tensor([1996.6171, 3189.2651, 4269.5513, 1545.9236, 4410.1431]), W: tensor([30.7337, 35.2348, 59.1911]), b: 1.2239, Cost: 83.822998
Epoch  300/1000 hypothesis: tensor([1988.3447, 3186.3164, 4280.6699, 1545.6133, 4406.2095]), W: tensor([30.1901, 35.0457, 59.7720]), b: 1.2082, Cost: 6.701727
Epoch  400/1000 hypothesis: tensor([1985.8756, 3185.6016, 4283.7368, 1545.3737, 4405.1768]), W: tensor([30.0410, 34.9839, 59.9365]), b: 1.2029, Cost: 0.579013
Epoch  500/1000 hypothesis: tensor([1985.1711, 3185.4065, 4284.5981, 1545.2971, 4404.8896]), W: tensor([29.9991, 34.9660, 59.9830]), b: 1.2009, Cost: 0.091294
Epoch  600/1000 hypothesis: tensor([1984.9717, 3185.3516, 4284.8413, 1545.2748, 4404.8091]), W: tensor([29.9874, 34.9610, 59.9961]), b: 1.1997, Cost: 0.052307
Epoch  700/1000 hypothesis: tensor([1984.9155, 3185.3359, 4284.9097, 1545.2684, 4404.7866]), W: tensor([29.9840, 34.9596, 59.9998]), b: 1.1988, Cost: 0.049147
Epoch  800/1000 hypothesis: tensor([1984.8995, 3185.3315, 4284.9292, 1545.2662, 4404.7803]), W: tensor([29.9831, 34.9592, 60.0009]), b: 1.1979, Cost: 0.048838
Epoch  900/1000 hypothesis: tensor([1984.8959, 3185.3293, 4284.9351, 1545.2664, 4404.7783]), W: tensor([29.9828, 34.9592, 60.0012]), b: 1.1971, Cost: 0.048723
Epoch 1000/1000 hypothesis: tensor([1984.8950, 3185.3291, 4284.9355, 1545.2659, 4404.7783]), W: tensor([29.9828, 34.9592, 60.0012]), b: 1.1963, Cost: 0.048662
```