---
title:  "2.2 Softmax Regression"
excerpt: 

categories:
  - Robot
tags:
  - [Machine Learning]

toc: true
toc_sticky: true
 
date: 2022-08-30
last_modified_at: 2022-08-30

use_math: true
published: true
---

<br>

새 무기를 산 지훈이는 못 가 봤던 던전에 도전할 수 있게 되었다. 이 던전은 몬스터가 무한으로 등장하며 죽기 전까지 버틴 시간과 토벌한 몬스터의 수를 토대로 A부터 C까지의 랭크를 부여한다.

|Time ($x_1$)|Killed ($x_2$)|Rank ($y$)|
|---|---|---|
|10|7|A|
|8|5|A|
|4|5|B|
|3|3|B|
|2|4|B|
|2|2|C|
|12|1|C|


<br>

***

### 2.2.1 Logistic Regression Returns

Sec 2.1의 Logistic regression을 그림으로 표현하면 다음과 같다.

<p align="center"><img src="/assets/image/machine_learning/ml/ch2/220830_1.svg" width="" height="" title="" alt=""><br/></p>

Input $X$가 들어오면 weight $W$를 갖는 unit 내에서 계산한다. 그 결과가 $z$이고, 이 $z$를 sigmoid function에 통과시켜 $0$과 $1$ 사이의 값을 갖는 $\overline{Y}$를 얻는다. 이때 $Y$는 real value, $\overline{Y}$는 predictive value를 말한다.

<br>

Logistic regression을 직관적으로 보면, 다음과 같은 데이터들이 있을 때 이들을 분류해낼 수 있는 line을 학습시키는 것과 같다. 

<p align="center"><img src="/assets/image/machine_learning/ml/ch2/220830_2.svg" width="" height="" title="" alt=""><br/></p>

<br>

***

### 2.2.1 Multinomial Logistic Regression

자, 이번에는 지훈이의 클리어 실적으로부터 랭크를 분류해보도록 하자. 그래프를 그리면 다음과 같은 모습일 것이다.

<p align="center"><img src="/assets/image/machine_learning/ml/ch2/220830_3.svg" width="" height="" title="" alt=""><br/></p>

이들을 어떻게 분류할 것인가?

<br>

Linear regression을 떠올리자면 먼저 $A$이거나 $A$가 아닌 case로 나눌 수 있겠다. 이에 따른 model이 하나 만들어질 것이다.

그럼 $B$인지 아닌지 분류할 수 있는 model도, $C$인지 아닌지 분류할 수 있는 model도 있겠다.

<p align="center"><img src="/assets/image/machine_learning/ml/ch2/220830_4.svg" width="" height="" title="" alt=""><br/></p>

<br>

각각의 linear regression model에서 input data는 다음과 같은 계산을 거친다.

$$\begin{bmatrix}
w_1 & w_2 & w_3
\end{bmatrix} \begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix} = \begin{bmatrix}
w_1x_1 + w_2x_2 + w_3x_3
\end{bmatrix}$$

3개의 model이 있으므로, 우리는 이를 행렬곱을 사용해 표현할 수 있다.

$$\begin{bmatrix}
w_{A1} & w_{A2} & w_{A3} \\
w_{B1} & w_{B2} & w_{B3} \\
w_{C1} & w_{C2} & w_{C3}
\end{bmatrix} \begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix} = \begin{bmatrix}
w_{A1}x_1 + w_{A2}x_2 + w_{A3}x_3 \\
w_{B1}x_1 + w_{B2}x_2 + w_{B3}x_3 \\
w_{C1}x_1 + w_{C2}x_2 + w_{C3}x_3
\end{bmatrix} = \begin{bmatrix}
\overline{y}_A \\ \overline{y}_B \\ \overline{y}_C
\end{bmatrix}$$

이렇게 하면 세 logistic regression들이 마치 독립적으로 동작하는 것과 같은 형태의 model을 얻을 수 있다.

그런데 이렇게 하면 각 model마다 sigmoid를 한 번씩, 총 세 번이나 계산해야 하니 조금 오래 걸릴 것 같다. 보다 효율적인 방법이 있을 것이다.

<br>

***

### 2.2.1 Softmax Classifier

Sec 2.1의 마지막 부분을 떠올려 보자. 우리는 logistic regression model의 evaluation을 위해 x_train값을 우리가 만든 모델에 집어넣어 0에서 1 사이의 확률값을 얻었다.

Multinomial의 경우에도 이런 결과가 나올 수 있으면 좋겠다. $\overline{y}_A$는 높은 값이, $\overline{y}_B$와 $\overline{y}_C$는 낮은 값이 나오게 된다면 해당 case를 A랭크로 분류할 수 있을 텐데.

<br>

데이터의 결과값 $\overline{y}$를 0과 1 사이의 확률적인 값으로 바꾸어주는 함수가 바로 <span style="color:red">**Softmax**</span>이다. 즉, multinomial case에서 sigmoid와 같은 역할을 하는 녀석이다.

Softmax의 정의는 다음과 같다.

$$
S(y_i) = \frac{e^{y_i}}{\sum_j e^{y_j}}
$$

즉 전체 값으로 해당 값을 나눈 것과 같다.

<p align="center"><img src="/assets/image/machine_learning/ml/ch2/220830_5.svg" width="" height="" title="" alt=""><br/></p>

마지막으로 <span style="color:red">**One-Hot Encoding**</span>을 사용하여 가장 큰 값을 $1$로, 나머지를 $0$으로 만들어 주면 끝이 난다.

<br>

***

### 2.2.1 Cost Function

이제 cost function만 해결하면 되겠다. Cost function은 우리가 예측한 값과 실제 값이 얼마나 차이가 나는지를 알려주는 지표였다. 이 cost function이 최소화가 되면 학습이 끝나는 것이었다.

Classification에서는 0에서 1 사이의 확률값을 비교하게 된다.

