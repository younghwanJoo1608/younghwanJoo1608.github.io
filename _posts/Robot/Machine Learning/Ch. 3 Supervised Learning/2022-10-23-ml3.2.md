---
title:  "3.2 Support Vector Machine"
excerpt: 

categories:
  - Robot
tags:
  - [Machine Learning]

toc: true
toc_sticky: true
 
date: 2022-10-23
last_modified_at: 2022-10-23

use_math: true
published: true
---

<br>

***

### 3.2.1 Support Vector Machine

Support Vector Machine은 분류를 위한 기준선을 정의한다. 분류되지 않은 데이터가 나타나면 기준선을 기준으로 경계의 어느쪽에 속할지 분류한다.

<br>

라이브러리 호출 및 데이터 준비. 이 예시에서는 tensorflow가 필요하므로, anaconda prompt에서 다음 명령어를 입력.

```
pip install tensorflow
```

<script src="https://gist.github.com/younghwanJoo1608/23b1dead5cc65c730c7cddf67c3bfc4c.js"></script>

```cpp
 - Line 8 : TF_CPP_MIN_LOG_LEVEL라는 환경변수로 로깅을 제어한다. 기본값은 0으로 모든 로그가 표시된다.
 1은 INFO log를 필터링, 2는 WARNING log를 필터링, 3은 ERROR log를 필터링. 
```

여기서도 마찬가지로 iris dataset을 사용한다. 다만 이번에는 data 파일을 불러오지 않고 `sklearn`에서 제공하는 iris dataset을 사용해 보자.

<script src="https://gist.github.com/younghwanJoo1608/88ad8ca8d8e170a5c5e4e9c075aaca99.js"></script>

<br>

SVM 모델을 생성하고 훈련시킨다. Test dataset을 이용해 예측을 수행하자.

<script src="https://gist.github.com/younghwanJoo1608/16d0fa9ab1c89a59e6d23a61ae107f87.js"></script>

```cpp
 - Result : 
 정확도: 0.988889
```

<br>

***

### 3.2.2 Kernel for SVM

위 코드의 Line 1에서 SVC method 내부의 값을 보자. 이 parameter들은 SVM의 kernel을 결정한다.

비선형 문제를 해결하기 위한 가장 기본적인 방법은 저차원 데이터를 고차원으로 보내는 것이다. 여기서 사용하는 것이 바로 <span style="color:red">**kernel trick**</span>.

<br>

#### Linear Kernel

선형으로 분류할 수 있는 데이터에 적용한다.

$$
K(a, b) = a^T \cdot b
$$

Linear kernel은 가장 기본적인 kernel. Kernel trick을 사용하지 않겠다는 것과 같다.

<br>

#### Polynomial Kernel

실제로는 feature를 추가하지 않지만, polynomial feature를 많이 추가한 것과 같은 결과를 얻는다.

$$
K(a,b) = (\gamma a^T \cdot b)^d
$$

여기서 $\gamma$와 $d$:차원은 hyperparameter.

<br>

#### Gaussian RBF Kernel

대표적인 비선형 kernel. 무한차원으로 매핑한다.

$$
K(a,b) = \exp(\gamma \Vert a-b \Vert^2)
$$

여기서 $\gamma$는 hyperparameter.

<br>

$\gamma$는 결정 경계를 얼마나 유연하게 할 지를 지정한다. Training data에 민감하게 하고 싶다면 $\gamma$를 높게 지정한다. 단 이 경우 결졍 경계가 curve 형태가 되며 overfitting이 일어날 수 있다.

끝으로 $C$값은 오류를 얼마나 허용하는지를 나타낸다. $C$값이 크면 hard margin, 작으면 soft margin이다. 이에 대한 수학적 이론은 이후에.