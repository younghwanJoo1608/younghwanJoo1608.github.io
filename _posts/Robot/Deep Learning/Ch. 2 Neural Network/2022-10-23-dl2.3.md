---
title:  "2.3 Loss Function"
excerpt: 

categories:
  - Robot
tags:
  - [Machine Learning]

toc: true
toc_sticky: true
 
date: 2022-10-23
last_modified_at: 2022-10-23

use_math: true
published: true
---

<br>

Gradient descent method는 learning rate($\eta$)와 loss function의 순간 기울기를 사용해 오차를 최소화시키는 방향으로 weight를 업데이트한다. 이때 오차를 구하는 방법이 loss function. 학습을 통해 얻은 데이터 추정치와 실제 데이터는 얼마나 차이가 나는가?

여기서는 대표적인 두 loss function을 소개.

<br>

***

### 2.3.1 Mean Squared Error(MSE)

MSE는 실제 값과 예측 값의 차이를 제곱해 평균을 낸 것. Regression에서 자주 사용된다.

$$
MSE = \frac{1}{n}\sum^n_{i=1} (y_i-\hat{y_i})^2
$$

<center>$\hat{y_i}$ : 신경망의 출력(추정값)</center>
<center>$y_i$ : 정답 label</center>
<center>$i$ : data의 dimension</center>

pytorch에서는 다음과 같이 쓴다.

<script src="https://gist.github.com/younghwanJoo1608/bce17bc81b47254f6465502c90ee97a0.js"></script>

<br>

***

### 2.3.2 Cross Entropy Error(CEE)

CEE는 classification 문제에서 one-hot encoding 했을 때만 사용한다. Classification에서는 데이터의 출력이 0 또는 1이므로 sigmoid 함수를 사용하는데, 이때 MSE를 사용하면 매끄럽지 못한 그래프가 출력된다.

이는 자연상수 $e$ 때문인데, CEE는 $e$에 반대되는 자연로그를 model의 출력값에 취한다.

$$
CrossEntropy = -\sum^n_{i=1} y_i \log \hat{y_i}
$$

<center>$\hat{y_i}$ : 신경망의 출력(추정값)</center>
<center>$y_i$ : 정답 label</center>
<center>$i$ : data의 dimension</center>

pytorch에서는 다음과 같이 쓴다.

<script src="https://gist.github.com/younghwanJoo1608/15dd91463b2f3437e5ea5e400ffb9de8.js"></script>