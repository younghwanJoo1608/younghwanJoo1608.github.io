---
title:  "2.5 Problem in DL"
excerpt: 

categories:
  - Robot
tags:
  - [Machine Learning]

toc: true
toc_sticky: true
 
date: 2022-10-23
last_modified_at: 2022-10-23

use_math: true
published: true
---

<br>

***

### 2.5.1 Overfitting

<span style="color:red">**Overfitting**</span>은 데이터를 과도하게 학습해서 발생하는 문제이다. 예측값과 실제 값 사이의 오차는 감소하지만, 새롭게 들어오는 데이터에 대해서는 오차가 증가할 가능성이 있다.

아래 예시에서 검은색은 일반 model을, 초록색은 overfitted model을 나타낸다.

<p align="center"><img src="/assets/image/machine_learning/dl/ch2/221023_2.svg" width="" height="" title="" alt=""><br/></p>

<br>

Overfitting을 해결하는 대표적인 방법은 <span style="color:red">**dropout**</span>이다. 말 그대로, 학습 과정 중에서 임의로 일부 노드를 학습에서 제외시키는 것이다.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>

<p align="center"><img src="/assets/image/machine_learning/dl/ch2/221023_3.svg" width="" height="" title="" alt=""><br/></p>

<br>

다음은 pytorch에서 dropout을 구현한 예시이다.

<script src="https://gist.github.com/younghwanJoo1608/49aa1a94ebdc9e4947997b8969891f7c.js"></script>

<br>

***

### 2.5.2 Gradient Vanishing

Hidden layer가 많아지는 NN에서는 backpropagation 과정에서 output layer로부터 hidden layer로 전달될 때 오차가 크게 줄어들게 된다.

즉 gradient가 소멸되면서 weight의 학습량이 0에 가까워져 학습이 제대로 이루어지지 못하고 그대로 수렴해버리게 된다.

Sigmoid나 hyperbolic tangent 함수 대신 ReLu를 사용하면 해결 가능.

<br>


***

<div class="footnotes"><ol>
<li class="footnote" id="fn:1">
<p>
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 15, 1 (January 2014), 1929–1958.
<a href="#fnref:1" title=""> ↩</a><p>