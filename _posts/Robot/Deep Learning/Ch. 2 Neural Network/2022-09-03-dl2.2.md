---
title:  "2.2 Implemeting 3-layer Neural Network"
excerpt: 

categories:
  - Robot
tags:
  - [Deep Learning]

toc: true
toc_sticky: true
 
date: 2022-09-03
last_modified_at: 2022-09-03

use_math: true
published: true
---

<br>

3층짜리 NN를 구현해 보자. Input layer, 2개의 hidden layer, Output layer로 구성된다.

<p align="center"><img src="/assets/image/machine_learning/dl/ch2/220903_13.svg" width="" height="" title="" alt=""><br/></p>

<br>

***

### 2.2.1 Signal Transportation

다음 notation을 쓰기로 하자.

<p align="center"><img src="/assets/image/machine_learning/dl/ch2/220906_1.svg" width="" height="" title="" alt=""><br/></p>

<br>

Input layer에서 1층의 첫 번째 뉴런으로 가는 신호를 살펴보자. Bias neuron을 추가하면 다음과 같다.

<p align="center"><img src="/assets/image/machine_learning/dl/ch2/220906_2.svg" width="" height="" title="" alt=""><br/></p>

이제 hidden layer의 neuron $a_1^{(1)}$의 값은

$$a_1^{(1)} = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1^{(1)}$$

행렬곱을 이용하면 1층 전체 neuron의 값을 알 수 있겠다.

$$\boldsymbol{A}^{(1)} = \boldsymbol{X}\boldsymbol{W}^{(1)} + \boldsymbol{B}^{(1)}$$

$$\boldsymbol{A}^{(1)} = \begin{pmatrix}
a_1^{(1)} & a_2^{(1)} & a_3^{(1)}
\end{pmatrix}, \quad \boldsymbol{X} = \begin{pmatrix} x_1 & x_2\end{pmatrix}, \quad \boldsymbol{B} = \begin{pmatrix}b_1^{(1)} & b_2^{(1)} & b_3^{(1)}\end{pmatrix} \\
\boldsymbol{W}^{(1)} = \begin{pmatrix} w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\ w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}\end{pmatrix}$$

<br>

<script src="https://gist.github.com/younghwanJoo1608/c821ca3f2a1c4d5d892be544a101cb96.js"></script>

```cpp
 - Result : 

[0.3 0.7 1.1]
```

<br>

이제 layer 1에서의 activation function 처리. Activation function $h()$에 의해 변환된 신호를 $z$로 표기하자. 코드는 sigmoid function을 사용했다.

<p align="center"><img src="/assets/image/machine_learning/dl/ch2/220906_3.svg" width="" height="" title="" alt=""><br/></p>

<script src="https://gist.github.com/younghwanJoo1608/2051e083d011d0230e0b95880b4c30c0.js"></script>\

```cpp
 - Result : 

[0.57444252 0.66818777 0.75026011]
```

<br>

자, 이제 1층에서 2층으로 넘어가는 것도 간단하다. 대신 input으로 $z$가 들어갈 뿐이다. 다음은 전체 3-layer Neural Network의 구현 코드이다. 2층에서 ouput layer로의 activation function은 identity function으로 하였다.

<script src="https://gist.github.com/younghwanJoo1608/3f397ee31082fd23fb21306f5d6b14d4.js"></script>

```cpp
 - Result : 

[0.31682708 0.69627909]
```

<br>

여기서 `forward()` 함수는 input signal을 output signal로 변환시킨다. 즉 신호가 순방향으로 전달되는 **forward propagation**을 나타낸 것이다.