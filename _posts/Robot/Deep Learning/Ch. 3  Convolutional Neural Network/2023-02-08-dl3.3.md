---
title:  "3.3 Transfer Learning"
excerpt: 

categories:
  - Robot
tags:
  - [Deep Learning]

toc: true
toc_sticky: true
 
date: 2023-02-08
last_modified_at: 2023-02-08
use_math: true
published: true
---

<br>

***

Deep learning에는 많은 양의 데이터셋이 필요하다. 그럴 시간도 돈도 없다면 어떻게 하지?

<br>

<span style="color:red">**Transfer Learning**</span>(전이 학습)이란 큰 데이터셋으로 미리 훈련된 모델의 가중치를 가져와 우리가 쓰고자 하는 곳에 맞게 보정해서 사용하는 것을 의미한다. 이때 미리 훈련된 모델을 <span style="color:red">**network**</span>라고 한다.

이로부터 비교적 적은 dataset으로도 문제를 해결할 수 있게 된다.

***

### 3.3.1 Feature Extractor

<span style="color:red">**Feature Extractor**</span>(특성 추출)이란 사전 훈련된 model을 가져와 마지막 fully connected layer만 새로 만드는 기법을 말한다.

즉, 학습할 때에는 마지막, category를 결정하는 부분만을 학습하고 나머지 layer들은 학습되지 않도록 하는 것이다.

<br>

이미지 분류의 예시를 들자. Feature extractor는 다음 두 부분으로 구성된다.

- Convolutional Layer : convolutional layer 및 pooling layer로 구성.
- Data Classifier(Fully Connected Layer) : 추출된 feature를 입력받아 image에 대한 class를 분류하는 부분.

사전 훈련된 network의 convolutional layer(가중치는 고정되어 있다.)에 새 data를 통과시켜, 그 output을 classifier에서 훈련시키는 것!

<br>

사전 훈련된 model은 다음과 같은 것들을 사용한다.

- ImageNet
- Xception
- Inception V3
- ResNet 50

<p align="center"><img src="/assets/image/machine_learning/dl/ch3/230208_1.jpg" width="" height="" title="" alt=""><br/></p>

<br>

예시를 보이기 전 필요한 라이브러리 설치.

```bash
pip install opencv-python
```

<br>

<script src="https://gist.github.com/younghwanJoo1608/44a45798cc288b367cd87c895dbc7f9b.js"></script>


```py
Line 5 : OpenCV 라이브러리

Line 9 : Computer vision을 위한 패키지

Line 10 : 데이터 전처리를 위한 패키지

Line 11 : pytorch network 사용을 위한 패키지
```

<br>

다음은 전처리 방법을 정의하는 코드이다.

<script src="https://gist.github.com/younghwanJoo1608/11e91a811070991859bda4fecd19636a.js"></script>

```py
Line 3~9 : torchvision.transform은 이미지 데이터를 변환해 newtork의 input으로 사용할 수 있도록 변환한다. Parameter들은 다음과 같다.
 - Resize : 이미지의 크기 조정
 - RandomResizedCrop : 이미지를 랜덤한 비율로 자른 후 데이터 크기를 조정해 데이터를 확장하는 역할을 한다. 이에 반해 Resize는 convolution layer 통과를 위해 이미지 크기를 조정한다.
 - RandomHorizontalFlip : 이미지를 랜덤하게 수평으로 뒤집는다.
 - ToTensor : 이미지 데이터를 텐서로 변환한다.

Line 10~13 : data.ImageFolder는 데이터로더가 불러올 데이터와 방법을 정의한다.
 - data_path : 데이터가 위치한 경로
 - transform : 이미지 데이터 전처리

Line 14~19 : DataLoader는 데이터를 불러오는 부분으로, Line 10에서 정의한 ImageFolder(train_dataset)을 데이터로더에 할당한다.
 - train_dataset : 데이터셋 지정
 - batch_size : 한 번에 불러올 데이터양 결정
 - num_workers : 데이터를 불러올 때 사용할 하위 프로세스 개수
 - shuffle : 데이터를 무작위로 섞을 것인가?

---
Result : 
385
```

<br>

불러 온 이미지를 출력해 보자. 

<script src="https://gist.github.com/younghwanJoo1608/89ecf6f834da2bc720e5c3402ceea6fe.js"></script>

```py
Line 3 : iterator 사용. next()는 iterator가 다음에 출력해야 할 요소를 반환한다. 따라서 train_loader의 iterator를 next()에 전달해 차례로 꺼낼 수 있게 된다.

Line 4 : 개와 고양이에 대한 class

Line 8 : label 정보를 함께 출력
```

<p align="center"><img src="/assets/image/machine_learning/dl/ch3/230208_2.png" width="" height="" title="" alt=""><br/></p> <sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>

<br>

자. 데이터는 준비되었다. 이제 pre-trained network가 필요하다. ResNet18을 사용하자.

<script src="https://gist.github.com/younghwanJoo1608/df0118ef4a6234d8cd8fc40799c8d1fb.js"></script>

```py
Line 1 : pretrained=True는 사전 학습된 가중치를 사용하겠다는 것. 아무 parameter도 넣지 않으면 무작위 가중치의 모델을 구성한다.
```

이렇게 내려받은 Resnet18의 convolutional layer는 사용하되 parameter 학습은 진행하지 않을 것이다.

<script src="https://gist.github.com/younghwanJoo1608/0f5fafee3c2d14538be6c8d3354855d5.js"></script>

```py
Line 4 : model 일부(convolutional & pooling layer)를 고정하고, backpropagation 중 parameter의 변화를 계산하지 않는다!
```

가져온 ResNet18에 fully connected layer를 추가할 것이다. 여기서는 개와 고양이를 분류하는 용도로 쓰이게 될 것이다.

<script src="https://gist.github.com/younghwanJoo1608/def814b29c4938b54a696269e3e32f72.js"></script>

```py
Line 1 : 512는 input size, 2는 class가 2개라는 의미.

Line 3 : model.named_parameters()는 model에 접근해 paramater 값을 가져온다.

---
Result : 
fc.weight tensor([[-0.0245,  0.0340, -0.0173,  ...,  0.0141, -0.0203,  0.0080],
        [ 0.0071,  0.0301, -0.0211,  ...,  0.0320, -0.0322, -0.0001]])
fc.bias tensor([0.0037, 0.0401])
```

<br>

학습 준비를 시작하자. Model 객체 생성 후 cost function을 정의한다.

<script src="https://gist.github.com/younghwanJoo1608/31977a25f0a19bc252c923e0e0661994.js"></script>

```py
Line 1 : model 객체 생성

Line 3~4 : Convolutional layer의 weight 고정

Line 7~8 : fully connected layer는 학습시킬 것.

Line 10~11 : optimizer와 cost function 정의.

---
Result : 
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
```

Resnet18 모델 마지막에 fully connected layer가 추가되어 있는 것을 확인할 수 있다.

<br>

Training을 위한 함수를 만들자.

<script src="https://gist.github.com/younghwanJoo1608/23b9a003624e594ac56ade0f9207f80c.js"></script>

```py
Line 2 : PC의 현재 시간

Line 7 : epoch만큼 반복할 것.

Line 14 : DataLoader에 전달된 데이터만큼 반복할 것.

Line 19 : 기울기는 0으로 설정.

Line 20 : Feedforward 학습.

Line 22 : torch.max()는 tensor와 dim을 입력으로 받아, 해당 dim에 존재하는 tensor 열의 최댓값과 해당 최댓값이 위치하는 index를 반환한다. preds는 index를 가져온 것.

Line 23 : Backpropagation 학습.

Line 26 : 출력 결과와 label의 오차를 계산한 결과를 누적해서 저장한다.

Line 27 : 출력 결과와 label이 동일한지 확읺나 결과를 누적해서 저장한다.

Line 29~30 : 평균 오차와 평균 정확도를 계산한다.

Line 39 : 모델 재사용을 위해 저장.

Line 42 : 학습 시간을 계산.

Line 45 : 모델 정확도와 오차를 반환.
```

Fully connected layer는 학습을 하도록 설정한 것을 잊지 말자. 학습을 통해 얻은 parameter를 optimizer에 전달해야 한다.

<script src="https://gist.github.com/younghwanJoo1608/abcbf36ff4f6cec80762216cf5f5628e.js"></script>

```py
Line 4 : parameter(weight, bias) 학습 결과를 저장.

Line 7 : 학습 결과를 optimizer에 전달
```

<br>

이제 모델을 학습시킬 때가 됐다! 전달되는 parameter는 (모델, 학습 데이터, cost function, optimizer, 장치(CPU 또는 GPU))이다.

<script src="https://gist.github.com/younghwanJoo1608/3f07c942f762bba6794c6d275014c5a8.js"></script>

```py
Result : 
Epoch 0/12
----------
Loss: 0.6201 Acc: 0.6208

Epoch 1/12
----------
Loss: 0.4092 Acc: 0.8208

Epoch 2/12
----------
Loss: 0.3235 Acc: 0.8883

Epoch 3/12
----------
Loss: 0.3025 Acc: 0.8883

Epoch 4/12
----------
Loss: 0.3570 Acc: 0.8312

Epoch 5/12
----------
Loss: 0.3203 Acc: 0.8416

Epoch 6/12
----------
Loss: 0.3251 Acc: 0.8545

Epoch 7/12
----------
Loss: 0.2056 Acc: 0.9273

Epoch 8/12
----------
Loss: 0.2248 Acc: 0.8987

Epoch 9/12
----------
Loss: 0.2626 Acc: 0.8857

Epoch 10/12
----------
Loss: 0.2064 Acc: 0.9039

Epoch 11/12
----------
Loss: 0.2228 Acc: 0.8935

Epoch 12/12
----------
Loss: 0.1959 Acc: 0.9117

Training complete in 0m 56s
Best Acc: 0.927273
```

정확도는 92.7%. 꽤 쓸만하다. Training에 쓰인 파일들은 저장해 두었으니 언제든 꺼내서 쓸 수 있다.

자, 이제 test용 데이터를 꺼내 와서 model의 정확도를 평가할 시간이다.

***

<div class="footnotes"><ol>
<li class="footnote" id="fn:1">
<p>
뭔가 애매한 사진들도 섞여있긴 하지만...
<a href="#fnref:1" title=""> ↩</a><p>
